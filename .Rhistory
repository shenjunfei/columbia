update(this_moment, hours = 10, minutes = 16, seconds = 0)
update(this_moment, hours = 10, minutes = 16, seconds = 0)
info()
update(this_moment, hours = 10, minutes = 16, seconds = 0)
skip()
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
arrive <- nyc + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, tzone = "Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?new_interval
how_long <- new_interval(last_time, arrive, tzone = attr(last_time, "Singapore"))
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
library(twitteR)
key <- "6iJMlFVaHjebp8MTnuhX7PSvZ"
secret <- "4beQ8LVZggujwxVHiIkyf4EWTZ1U55V14uDVzrCr90v1Z6bovM"
setup_twitter_oauth(key, secret)
token <- "3024233079-ok7pL5tkFoHNG80VY4aTDl3rMp87C0SL7ftv6M7"
token_secret <- "iEvWDVDEPWPKWeqbHv47cZvh1DdQ8kKb4UKA031nb2ndP"
setup_twitter_oauth(key, secret, access_token = token, access_secret = token_secret)
showStatus("123")
getUser("ShenJunfei")
getUser("EmWatson")
getUser("geoffjentry")
getUser("BarackObama")
userTimeline("EmWatson", n=100)
getUser("EmWatson")
getFollowers()
getFollowers(10)
?getFollowers
??getFollowers
favorites("EmWatson", n = 20)
ewf <- favorites("EmWatson", n = 20)
data.frame(ewf)
ewf[1]
dm <- dmFactory$new(text=􏰇foo􏰇, recipientSN=􏰇blah􏰇)
?getFollowers
?getFollowers("EmWatson")
getFollowers("EmWatson")
searchTwitter("@BarackObama", 10, lang="en")
em <- getUser("EmWatson")
em$getFollowers
em$getFollowers()
em$getFollowersCount()
em$getFriends(n=10)
em$getFriends(n=5)
em$getLongitude
em$getLongitude()
f <- em$getFriends()
em$getDescription
em$getDescription()
em$getFriends(n=1)
em$getFriends(n=5)
em$statusesCount()
em$statusesCount
class(em$statusesCount)
class(em$statusesCount[1])
em$friendsCount()
em$friendsCount
em$created
class(em$created)
em$favouriteCount
em$favoriteCount
em$favoriteCount()
em$getFollow()
em$getFollow
em$getFollowCount()
em$getFollowCount
names(em)
dim(em)
em[1]
em$description
em$name
js <- getUser("ShenJunfei")
js <- getUser("ShenJunfei")
em <- getUser("EmWatson")
x <- c(5.1, 4.9, 4.7, 4.6, 5.0)
y <- c(3.5, 3.0, 3.2, 3.1, 5.4)
cor(x, y)
x <- c(7.0, 6.4, 6.9, 5.5, 6.5)
y <- c(3.2, 3.2, 3.1, 2.3, 2.8)
cor(x,y)
install.packages("rCharts")
install.packages("rCharts")
install.packages("base64enc")
install.packages("rCharts")
require(devtools)
install_github('rCharts', 'ramnathv')
library(devtools)
install.packages("devtools")
require(devtools)
install_github('rCharts', 'ramnathv')
library(rCharts)
library(ggplot)
library(ggplots)
library(ggplot2)
install.pakcages("ggplot2")
install.packages("ggplot2")
install_github("ropensci/plotly")
library(plotly)
load(couseraData.rda)
load(courseraData.rda)
load("courseraData.rda")
load("courseraData.rda")
library(plotly)
library(shiny)
instaall.packages("shiny")
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
install.packages("yhatr")
library(yhatr)
install.packages("yhatr")
d1 <- c(1, 0.05, 0.9, .8)
d2 <- c(0.05, 1, 0.01, .5)
d3 <- c(0.9, 0.01, 1, 0.7)
d4 <- c(0.8, 0.5, .7, 1)
sqrt(sum(d4 - d1)^2)
sqrt(sum(d4 - d2)^2)
sqrt(sum(d4 - d3)^2)
library(twitteR)
key <- "6iJMlFVaHjebp8MTnuhX7PSvZ"
secret <- "4beQ8LVZggujwxVHiIkyf4EWTZ1U55V14uDVzrCr90v1Z6bovM"
token <- "3024233079-ok7pL5tkFoHNG80VY4aTDl3rMp87C0SL7ftv6M7"
token_secret <- "iEvWDVDEPWPKWeqbHv47cZvh1DdQ8kKb4UKA031nb2ndP"
myName <- "ShenJunfei"
setup_twitter_oauth(key, secret, access_token = token, access_secret = token_secret)
me <- getUser(myName)
install.packages("praise")
library(praise)
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
praise()
install.packages("Rweibo", repos = "http://R-Forge.R-project.org")
library(Rweibo)
?registerApp
registerApp(app_name=J.Shen, app_key=4268054691, app_secret=05543d9aed56db4e6857e618bee4ff86)
registerApp(J.Shen, 4268054691, 05543d9aed56db4e6857e618bee4ff86)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
?Rweibo
??Rweibo
statuses.count()
roauth <- createOAuth("J.Shen", "Q_E_D")
roauth <- createOAuth("J.Shen", "Q_E_D")
roauth <- createOAuth("J.Shen", "Q_E_D", authorize=TRUE, login=FALSE, username="497739328@qq.com", password="371620sjf")
library(Rweibo)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
library(Rweibo)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
roauth <- createOAuth(app_name="J.Shen", access_name="rweibo")
require(Rweibo)
registerApp(app_name = "SNA3", "********", "****************")
roauth <- createOAuth(app_name = "SNA3", access_name = "rweibo")
require(Rwordseg)
install.packages(Rwordseg)
??Rwordseg
?Rwordseg
??Rwordseg
install.packages("Rwordseg", repos = "http://R-Forge.R-project.org")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
library(tm)
install.packages("tm")
install.packages("rJava")
??tmcn
?tmcn
install.packages("tmcn")
install.packages("tmcn", repos = "http://R-Forge.R-project.org")
install.packages("tmcn")
install.packages("~/Downloads/tmcn_0.1-3.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn_0.1-4.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn_0.1-4.tar", repos=NULL, type="source")
install.packages("~/Downloads/tmcn", repos=NULL, type="source")
library(tmcn)
library(tmcn)
?tmcn
??tmcn
Rweibo
install.packages("~/Downloads/Rweibo", repos=NULL, type="source")
library(Rweibo)
library(tmcn)
registerApp("J.Shen", "4268054691", "05543d9aed56db4e6857e618bee4ff86")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
install.packages("bitops")
install.packages("bitops")
install.packages("XML")
install.packages("Rjson")
install.packages("rjson")
install.packages("RCurl")
install.packages("digest")
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
library(Rweibo)
roauth <- createOAuth(app_name="J.Shen", access_name="Q_E_D")
??registerApp
install.packages('stringi')
version()
getVersion()
verson
version
install.packages("quanteda")
require(quanteda)
summary(ie2010Corpus)
ieDfm <- dfm(ie2010Corpus, ignoredFeatures = c(stopwords("english"), "will"), stem = TRUE)
topfeatures(ieDfm)
plot(ieDfm, min.freq=25, random.order=FALSE)
warnings()
getwd()
help(package="quanteda")
summary(ukimmigTexts)
str(ukimmigTexts)
ukimmigTexts
encoding(ukimmigTexts)
encoding(encodedTexts)
endodedTexts
encodedTexts
immigCorpus <- corpus(ukimmigTexts, notes="Created as part of a demo.")
immigCorpus
immigCorpus[1]
immigCorpus[9]
immigCorpus[4]
immigCorpus[3]
immigCorpus[2]
docvars(immigCorpus) <- data.frame(party = docnames(immigCorpus), year = 2010)
summary(immigCorpus)
kwic(immigCorpus, "deport", window = 3)
kwic(immigCorpus, "illegal immig*", window = 3)
immigDfm <- dfm(subset(immigCorpus, party=="BNP"))
plot(immigDfm)
immigDfm <- dfm(subset(immigCorpus, party=="BNP"), ignoredFeatures = stopwords("english"))
plot(immigDfm, random.color = TRUE, rot.per = .25, colors = sample(colors()[2:128], 5))
immigCorpusSent <- changeunits(immigCorpus, to = "sentences")
immigCorpusSent
immigCorpusSent[1]
summary(immigCorpusSent, 20)
txt <- "#TextAnalysis is MY <3 4U @myhandle gr8 #stuff :-)"
tokenize(txt, removePunct=TRUE)
tokenize(txt, removePunct=TRUE, removeTwitter=TRUE)
(toks <- tokenize(toLower(txt), removePunct=TRUE, removeTwitter=TRUE))
str(toks)
toks
toks[1]
(sents <- tokenize(ukimmigTexts[1], what = "sentence", simplify = TRUE)[1:5])
sens
sents
tokenize(ukimmigTexts[1], what = "character", simplify = TRUE)[1:100]
summary(inaugCorpus)
presDfm <- dfm(inaugCorpus)
presDfm
docnames(presDfm)
presDfm <- dfm(inaugCorpus, groups="President")
presDfm
docnames(presDfm)
data(iebudgetsCorpus, package = "quantedaData")
summary(iebudgetsCorpus, 10)
data(iebudgetsCorpus, package = "quanteData")
library(quantedaData)
install.packages("quantedaData")
require(quanteda)
data(iebudgetsCorpus, package = "quantedaData")
data(iebudgetsCorpus, package = "quanteda")
??quantedaData
devtools::install_github("kbenoit/quantedaData")
data(iebudgetsCorpus, package = "quantedaData")
summary(iebudgetsCorpus, 10)
ieFinMin <- subset(iebudgetsCorpus, number=="01" & debate == "BUDGET")
ieFinMin
ieFinMin[1]
summary(ieFinMin)
dfmFM <- dfm(ieFinMin)
plot(2008:2012, lexdiv(dfmFM, "C"), xlab="Year", ylab="Herndan's C", type="b",
main = "World's Crudest Lexical Diversity Plot")
data(SOTUCorpus, package = "quantedaData")
fk <- readability(SOTUCorpus, "Flesch.Kincaid")
year <- lubridate::year(docvars(SOTUCorpus, "Date"))
library(lubridate)
install.packages("lubridate")
library(lubridate)
year <- lubridate::year(docvars(SOTUCorpus, "Date"))
year
require(ggplot2)
partyColours <- c("blue", "blue", "black", "black", "red", "red")
p <- ggplot(data = docvars(SOTUCorpus), aes(x = year, y = fk)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2, linetype=1, color="grey70", method = "loess", span = .34) +
xlab("") +
ylab("Flesch-Kincaid") +
geom_point(aes(colour = party)) +
scale_colour_manual(values = partyColours) +
geom_line(aes(), alpha=0.3, size = 1) +
ggtitle("Text Complexity in State of the Union Addresses") +
theme(plot.title = element_text(lineheight=.8, face="bold"))
quartz(height=7, width=12)
print(p)
presDfm <- dfm(inaugCorpus, ignoredFeatures = stopwords("english"))
similarity(presDfm, "1985-Reagan", n=5, margin="documents")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "cosine")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "Hellinger")
similarity(presDfm, c("2009-Obama" , "2013-Obama"), n=5, margin="documents", method = "eJaccard")
similarity(presDfm, c("fair", "health", "terror"), method="cosine")
txt <- "Hey @kenbenoit #textasdata: The quick, brown fox jumped over the lazy dog!"
(toks1 <- tokenize(toLower(txt), removePunct = TRUE))
tokenize(toLower(txt), removePunct = TRUE, ngrams = 2)
tokenize(toLower(txt), removePunct = TRUE, ngrams = c(1,3))
ngrams(tokens, c(1, 3, 5))
tokens <- tokenize(toLower("Insurgents killed in ongoing fighting."),
removePunct = TRUE, simplify = TRUE)
ngrams(tokens, c(1, 3, 5))
skipgrams(tokens, n = 2, k = 2, concatenator = " ")
skipgrams(tokens, n = 3, k = 2, concatenator = " ")
collocs2 <- collocations(inaugTexts, size = 2, method = "all")
head(collocs2, 20)
collocs3 <- collocations(inaugTexts, size = 3, method = "all")
head(collocs3, 20)
head(removeFeatures(collocs2, stopwords("english")), 20)
head(removeFeatures(collocs3, stopwords("english")), 20)
summary(inaugTexts)
summary(inaugTexts[1:5])
oneText <- inaugTexts[1]
oneText[2]
tmp <- inaugTexts[1:5]
length(inaugTexts)
length(oneText)
nchar(oneText)
nchar(inaugTexts[5:7])
?tokenize
tokens <- tokenize('Today is Thursday in Canberra. It is yesterday in London.')
tokens
vec <- c(one='This is text one', two='This, however, is the second text')
tokenize(vec)
tokenize(toLower(vec), removePunct = TRUE)
require(dplyr)
library(dplyr)
install.packages("dplyr")
library(dplyr)
require(dplyr)
inaugTokens <- tokenize(toLower(inaugTexts))
inaugTokens[2]
inaugDfm <- dfm(inaugTokens)
trimmedInaugDfm <- trim(inaugDfm, minDoc=5, minCount=10)
weightedTrimmedDfm <- weight(trimmedInaugDfm, type='tfidf')
require(dplyr)
inaugDfm2 <- dfm(inaugTokens) %>% trim(minDoc=5, minCount=10) %>% weight(type='tfidf')
methods(dfm)
methods(class = "tokenizedTexts")
summary(inaugTexts[52:57])
dv <- data.frame(Party = c('dem','dem','rep','rep','dem','dem'))
recentCorpus <- corpus(inaugTexts[52:57], docvars=dv)
summary(recentCorpus)
partyDfm <- dfm(recentCorpus, groups='Party', ignoredFeatures=(stopwords('english')))
wordcloud::comparison.cloud(t(as.matrix(partyDfm)))
require(quanteda)
s1 <- 'my example text'
length(s1)
nchar(s1)
s2 <- c('This is', 'my example text.', 'So imaginative.')
nchar(s2)
sum(nchar(s2))
which.max(nchar(inaugTexts))
inaguTexts
inaugTexts
which.min(nchar(inaugTexts))
s1 <- 'This file contains many fascinating example sentences.'
s1[6:9]
s1 <- 'This file contains many fascinating example sentences.'
substr(s1, 6,9)
names(inaugTexts)
s1 <- 'split this string'
strsplit(s1, 'this')
parts <- strsplit(names(inaugTexts), '-')
years <- sapply(parts, function(x) x[1])
parts
years
pres <-  sapply(parts, function(x) x[2])
paste('one','two','three')
paste('one','two','three', sep='_')
paste(years, pres, sep='-')
paste(years, pres, collapse='-')
tolower(s1)
toupper(s1)
tolower(c("This", "is", "Kεφαλαία Γράμματα"))
methods(toLower)
tolower(s1) == toupper(s1)
'apples'=='oranges'
c1 <- c('apples', 'oranges', 'pears')
'pears' %in% c1
grep('orangef', 'these are oranges')
require(quanteda)
summary(inaugTexts)
summary(inaugTexts[1:5])
inaugTexts
class(inaugTexts)
inaugTexts[1]
length(inaugTexts)
setwd("~/Documents/15fall/Thesis/Columbia")
data <- read.csv("data/final.csv", stringsAsFactors = FALSE)
data2 <- data.frame(source=rep(data$source, 2), target=rep(data$target, 2),
sourceLab1=c(data$sourceLab1, data$sourceLab2),
sourcePos1=c(data$sourcePos1, data$sourcePos2),
sourceLab2=c(data$sourceLab2, data$sourceLab3),
sourcePos2=c(data$sourcePos2, data$sourcePos3),
targetLab1=c(data$targetLab1, data$targetLab2),
targetPos1=c(data$targetPos1, data$targetPos2),
targetLab2=c(data$targetLab2, data$targetLab3),
targetPos2=c(data$targetPos2, data$targetPos3),
sourceFriends=rep(data$sourceFriends, 2),
targetFollowers=rep(data$targetFollwers, 2))
data2Sub <- data2[data2$sourceLab1 != "neutral" & data2$sourceLab2 != "neutral" &
data2$targetLab1 != "neutral" & data2$targetLab2 != "neutral", ]
model5 <- lm(sourcePos2 ~ targetPos2 + sourcePos1 + targetPos1 + targetFollowers, data=data2Sub)
summary(model5)
model6 <- lm(sourcePos2 ~ perFriend + sourcePos1 + targetPos1 + targetFollowers, data=data2Sub)
summary(model6)
data2Sub$perFriend <- data2Sub$targetPos2/data2Sub$sourceFriends
model6 <- lm(sourcePos2 ~ perFriend + sourcePos1 + targetPos1 + targetFollowers, data=data2Sub)
summary(model6)
hist(data2Sub$targetFollowers)
summary(data2Sub$targetFollowers)
sum(data2Sub$targetFollowers < 10000)
View(data)
profile <- read.csv("data/allProfile.csv")
View(profile)
summary(profile$followers_count)
hist(profile$followers_count)
sum(data2Sub$targetFollowers < 10000)
sum(profile$followers_count)
sum(profile$followers_count > 10000)
sum(profile$followers_count > 10)
sum(profile$followers_count > 100)
sum(profile$followers_count > 1000)
sum(profile$followers_count > 5000)
sum(profile$followers_count > 500)
sum(profile$followers_count > 10000)
sum(profile$followers_count < 100)
sum(profile$followers_count < 500)
sum(profile$followers_count < 1000)
sum(profile$followers_count < 2000)
sum(profile$followers_count < 100)
sum(profile$followers_count > 100 & profile$followers_count < 500)
sum(profile$followers_count <= 100)
sum(profile$followers_count > 100 & profile$followers_count =< 500)
sum(profile$followers_count > 100 & profile$followers_count <= 500)
sum(profile$followers_count > 500 & profile$followers_count <= 1000)
sum(profile$followers_count > 1000 & profile$followers_count <= 2000)
sum(profile$followers_count > 2000 & profile$followers_count <= 5000)
sum(profile$followers_count > 5000 & profile$followers_count <= 10000)
sum(profile$followers_count > 10000)
level <- c(["[0, 100]", "(100, 500]", "(500, 1000]", "(1000, 2000]", "(2000, 5000]", "(5000, 10000]", "(10000, )")
level <- c("[0, 100]", "(100, 500]", "(500, 1000]", "(1000, 2000]", "(2000, 5000]", "(5000, 10000]", "(10000, )")
count <- c(160, 697, 534, 510, 529, 378, 1294)
len(level)
length(level)
length(count)
profile$followers_count[profile$followers_count <= 1000]
hist(profile$followers_count[profile$followers_count <= 1000])
hist(profile$followers_count[profile$followers_count <= 5000])
sum(profile$followers_count > 100 & profile$followers_count <= 200)
sum(profile$followers_count > 200 & profile$followers_count <= 300)
sum(profile$followers_count > 300 & profile$followers_count <= 400)
sum(profile$followers_count > 400 & profile$followers_count <= 500)
sum(profile$followers_count > 500 & profile$followers_count <= 600)
sum(profile$followers_count > 200 & profile)
sum(profile$followers_count > 10000 & profile$followers_count <= 50000)
sum(profile$followers_count > 50000 & profile$followers_count <= 100000)
sum(profile$followers_count > 100000)
sum(profile$followers_count > 10000 & profile$followers_count <= 20000)
level <- c("[0, 100]", "(100, 500]", "(500, 1000]", "(1000, 2000]", "(2000, 5000]",
"(5000, 10000]", "(10000, 50000]", "(50000, 100000]", "(100000, )")
count <- c(160, 697, 534, 510, 529, 378, 727, 208, 359)
length(level)
length(count)
data2Sub$targetPop <- data2Sub$targetFollowers <= 500
model7 <- lm(sourcePos2 ~ perFriend + sourcePos1 + targetPos1 + targetPop, data=data2Sub)
summary(model7)
followers <- data.frame(range=range, count=count)
range <- c("[0, 100]", "(100, 500]", "(500, 1000]", "(1000, 2000]", "(2000, 5000]",
"(5000, 10000]", "(10000, 50000]", "(50000, 100000]", "(100000, )")
followers <- data.frame(range=level, count=count)
library(ggplot)
library(ggplot2)
qplot(level, data=followers, geom="bar")
ggplot(data=followers, aes(x=range, y=count)) + geom_bar(stat="identity")
sum(count)
